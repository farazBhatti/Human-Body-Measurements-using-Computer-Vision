{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6773a511",
   "metadata": {},
   "source": [
    "# Setting Path to Repository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f32376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAMZA\\Human-Body-Measurements-using-Computer-Vision\n"
     ]
    }
   ],
   "source": [
    "cd Human-Body-Measurements-using-Computer-Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a937a773",
   "metadata": {},
   "source": [
    "# Defining Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2f52ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir=r'sample_data\\input\\arsalan2.jpeg'  #Path to Test Image\n",
    "height=168  #Height in centimeters\n",
    "model_dir = 'deeplab_model' #dir to save DeepLab model (For Image Segmentation)\n",
    "pretrain='models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b274dee",
   "metadata": {},
   "source": [
    "# Downloading PreTrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a281c536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrained Model Already Downloaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.exists(pretrain) is False:\n",
    "  print(\"Downloading PreTrained Model\")\n",
    "\n",
    "  !python -m wget https://people.eecs.berkeley.edu/~kanazawa/cachedir/hmr/models.tar.gz && tar -xf models.tar.gz\n",
    "else:\n",
    "    print('PreTrained Model Already Downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be950c83",
   "metadata": {},
   "source": [
    "# Downloading CustomBodyPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b247d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m wget https://github.com/farazBhatti/Human-Body-Measurements-using-Computer-Vision/files/5886235/customBodyPoints.txt -o data/customBodyPoints.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbff119",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03979707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAMZA\\Anaconda3\\envs\\body_m\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\HAMZA\\Anaconda3\\envs\\body_m\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\HAMZA\\Anaconda3\\envs\\body_m\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\HAMZA\\Anaconda3\\envs\\body_m\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\HAMZA\\Anaconda3\\envs\\body_m\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\HAMZA\\Anaconda3\\envs\\body_m\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os\n",
    "from io import BytesIO\n",
    "from absl import flags\n",
    "\n",
    "import src.config\n",
    "import sys\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "from six.moves import urllib\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2, pdb, glob, argparse\n",
    "import tensorflow as tf\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "from src.tf_smpl import projection as proj_util\n",
    "from src.tf_smpl.batch_smpl import SMPL\n",
    "from src.models import get_encoder_fn_separate\n",
    "import extract_measurements\n",
    "import cv2\n",
    "import skimage.io as io\n",
    "\n",
    "from src.util import renderer as vis_util\n",
    "from src.util import image as img_util\n",
    "from src.util import openpose as op_util\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b3385",
   "metadata": {},
   "source": [
    "# Defining Helper Functions and DeepLab model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a60e679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DeepLabModel(object):\n",
    "\t\"\"\"Class to load deeplab model and run inference.\"\"\"\n",
    "\n",
    "\tINPUT_TENSOR_NAME = 'ImageTensor:0'\n",
    "\tOUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n",
    "\tINPUT_SIZE = 513\n",
    "\tFROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
    "\n",
    "\tdef __init__(self, tarball_path):\n",
    "\t\t#\"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
    "\t\tself.graph = tf.Graph()\n",
    "\t\tgraph_def = None\n",
    "\t\t# Extract frozen graph from tar archive.\n",
    "\t\ttar_file = tarfile.open(tarball_path)\n",
    "\t\tfor tar_info in tar_file.getmembers():\n",
    "\t\t\tif self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
    "\t\t\t\tfile_handle = tar_file.extractfile(tar_info)\n",
    "\t\t\t\tgraph_def = tf.GraphDef.FromString(file_handle.read())\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\ttar_file.close()\n",
    "\n",
    "\t\tif graph_def is None:\n",
    "\t\t\traise RuntimeError('Cannot find inference graph in tar archive.')\n",
    "\n",
    "\t\twith self.graph.as_default():\n",
    "\t\t\ttf.import_graph_def(graph_def, name='')\n",
    "\n",
    "\t\tself.sess = tf.Session(graph=self.graph)\n",
    "\n",
    "\tdef run(self, image):\n",
    "\t\t\"\"\"Runs inference on a single image.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t  image: A PIL.Image object, raw input image.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t  resized_image: RGB image resized from original input image.\n",
    "\t\t  seg_map: Segmentation map of `resized_image`.\n",
    "\t\t\"\"\"\n",
    "\t\twidth, height = image.size\n",
    "\t\tresize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n",
    "\t\ttarget_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
    "\t\tresized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
    "\t\tbatch_seg_map = self.sess.run(\n",
    "\t\t\tself.OUTPUT_TENSOR_NAME,\n",
    "\t\t\tfeed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
    "\t\tseg_map = batch_seg_map[0]\n",
    "\t\treturn resized_image, seg_map\n",
    "\n",
    "def create_pascal_label_colormap():\n",
    "\t\"\"\"Creates a label colormap used in PASCAL VOC segmentation benchmark.\n",
    "\n",
    "\tReturns:\n",
    "\tA Colormap for visualizing segmentation results.\n",
    "\t\"\"\"\n",
    "\tcolormap = np.zeros((256, 3), dtype=int)\n",
    "\tind = np.arange(256, dtype=int)\n",
    "\n",
    "\tfor shift in reversed(range(8)):\n",
    "\t\tfor channel in range(3):\n",
    "\t\t  colormap[:, channel] |= ((ind >> channel) & 1) << shift\n",
    "\t\tind >>= 3\n",
    "\n",
    "\treturn colormap\n",
    "\n",
    "def label_to_color_image(label):\n",
    "\t\"\"\"Adds color defined by the dataset colormap to the label.\n",
    "\n",
    "\tArgs:\n",
    "\tlabel: A 2D array with integer type, storing the segmentation label.\n",
    "\n",
    "\tReturns:\n",
    "\tresult: A 2D array with floating type. The element of the array\n",
    "\t  is the color indexed by the corresponding element in the input label\n",
    "\t  to the PASCAL color map.\n",
    "\n",
    "\tRaises:\n",
    "\tValueError: If label is not of rank 2 or its value is larger than color\n",
    "\t  map maximum entry.\n",
    "\t\"\"\"\n",
    "\tif label.ndim != 2:\n",
    "\t\traise ValueError('Expect 2-D input label')\n",
    "\n",
    "\tcolormap = create_pascal_label_colormap()\n",
    "\n",
    "\tif np.max(label) >= len(colormap):\n",
    "\t\traise ValueError('label value too large.')\n",
    "\n",
    "\treturn colormap[label]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_image(img_path, json_path=None):\n",
    "    img = img_path#io.imread(img_path)\n",
    "    print(\"$$$$$$$\",img.shape)\n",
    "    if img.shape[2] == 4:\n",
    "        img = img[:, :, :3]\n",
    "\n",
    "    if json_path is None:\n",
    "        if np.max(img.shape[:2]) != 224:\n",
    "#            print('Resizing so the max image size is %d..' % config.img_size)\n",
    "            scale = (float(224) / np.max(img.shape[:2]))\n",
    "        else:\n",
    "            scale = 1.\n",
    "        center = np.round(np.array(img.shape[:2]) / 2).astype(int)\n",
    "        # image center in (x,y)\n",
    "        center = center[::-1]\n",
    "    else:\n",
    "        scale, center = op_util.get_bbox(json_path)\n",
    "\n",
    "    crop, proc_param = img_util.scale_and_crop(img, scale, center,\n",
    "                                               224)\n",
    "\n",
    "    # Normalize image to [-1, 1]\n",
    "    crop = 2 * ((crop / 255.) - 0.5)\n",
    "\n",
    "    return crop, proc_param, img\n",
    "\n",
    "\n",
    "def main(img_path, height, json_path=None):\n",
    "#    renderer = vis_util.SMPLRenderer(face_path='src/tf_smpl/smpl_faces.npy')\n",
    "    sess = tf.Session()\n",
    "    model = RunModel(sess=sess)\n",
    "#    cv2.imshow('input image for measurement extraction',img_path)\n",
    "#    cv2.waitKey(0)\n",
    "\n",
    "    input_img, proc_param, img = preprocess_image(img_path, json_path)\n",
    "    # Add batch dimension: 1 x D x D x 3\n",
    "    input_img = np.expand_dims(input_img, 0)\n",
    "\n",
    "    # Theta is the 85D vector holding [camera, pose, shape]\n",
    "    # where camera is 3D [s, tx, ty]\n",
    "    # pose is 72D vector holding the rotation of 24 joints of SMPL in axis angle format\n",
    "    # shape is 10D shape coefficients of SMPL\n",
    "    joints, verts, cams, joints3d, theta = model.predict(\n",
    "        input_img, get_theta=True)\n",
    "\n",
    "    extract_measurements.extract_measurements(height,verts[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6d0530",
   "metadata": {},
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e808710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunModel(object):\n",
    "    def __init__(self, sess=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          config\n",
    "        \"\"\"\n",
    "#        self.config = config\n",
    "\n",
    "        self.load_path = 'models/model.ckpt-667589'#config.load_path\n",
    "\n",
    "\n",
    "        # Data\n",
    "        self.batch_size = 1#config.batch_size\n",
    "        self.img_size = 224#config.img_size\n",
    " \n",
    "\n",
    "        self.data_format = 'NHMC'\n",
    "        self.smpl_model_path = 'models/neutral_smpl_with_cocoplus_reg.pkl'#config.smpl_model_path\n",
    "        \n",
    "        input_size = (self.batch_size, self.img_size, self.img_size, 3)\n",
    "        self.images_pl = tf.placeholder(tf.float32, shape=input_size)\n",
    "\n",
    "        # Model Settings\n",
    "        self.num_stage = 3#config.num_stage\n",
    "        self.model_type = 'resnet_fc3_dropout'#config.model_type\n",
    "\n",
    "        self.joint_type = 'cocoplus'#config.joint_type\n",
    " \n",
    "        # Camera\n",
    "        self.num_cam = 3\n",
    "        self.proj_fn = proj_util.batch_orth_proj_idrot\n",
    "\n",
    "        self.num_theta = 72        \n",
    "        # Theta size: camera (3) + pose (24*3) + shape (10)\n",
    "        self.total_params = self.num_cam + self.num_theta + 10\n",
    "\n",
    "        self.smpl = SMPL(self.smpl_model_path, joint_type=self.joint_type)\n",
    "\n",
    "\n",
    "\n",
    "        self.build_test_model_ief()\n",
    "\n",
    "        if sess is None:\n",
    "            self.sess = tf.Session()\n",
    "        else:\n",
    "            self.sess = sess\n",
    "        \n",
    "        # Load data.\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.prepare()        \n",
    "\n",
    "\n",
    "    def build_test_model_ief(self):\n",
    "        # Load mean value\n",
    "        self.mean_var = tf.Variable(tf.zeros((1, self.total_params)), name=\"mean_param\", dtype=tf.float32)\n",
    "\n",
    "        img_enc_fn, threed_enc_fn = get_encoder_fn_separate(self.model_type)\n",
    "        # Extract image features.        \n",
    "        self.img_feat, self.E_var = img_enc_fn(self.images_pl,\n",
    "                                               is_training=False,\n",
    "                                               reuse=False)\n",
    "        \n",
    "        # Start loop\n",
    "        self.all_verts = []\n",
    "        self.all_kps = []\n",
    "        self.all_cams = []\n",
    "        self.all_Js = []\n",
    "        self.final_thetas = []\n",
    "        theta_prev = tf.tile(self.mean_var, [self.batch_size, 1])\n",
    "        for i in np.arange(self.num_stage):\n",
    "            print('Iteration %d' % i)\n",
    "            # ---- Compute outputs\n",
    "            state = tf.concat([self.img_feat, theta_prev], 1)\n",
    "\n",
    "            if i == 0:\n",
    "                delta_theta, _ = threed_enc_fn(\n",
    "                    state,\n",
    "                    num_output=self.total_params,\n",
    "                    is_training=False,\n",
    "                    reuse=False)\n",
    "            else:\n",
    "                delta_theta, _ = threed_enc_fn(\n",
    "                    state,\n",
    "                    num_output=self.total_params,\n",
    "                    is_training=False,\n",
    "                    reuse=True)\n",
    "\n",
    "            # Compute new theta\n",
    "            theta_here = theta_prev + delta_theta\n",
    "            # cam = N x 3, pose N x self.num_theta, shape: N x 10\n",
    "            cams = theta_here[:, :self.num_cam]                \n",
    "            poses = theta_here[:, self.num_cam:(self.num_cam + self.num_theta)]\n",
    "            shapes = theta_here[:, (self.num_cam + self.num_theta):]\n",
    "\n",
    "            verts, Js, _ = self.smpl(shapes, poses, get_skin=True)\n",
    "\n",
    "            # Project to 2D!\n",
    "            pred_kp = self.proj_fn(Js, cams, name='proj_2d_stage%d' % i)\n",
    "            self.all_verts.append(verts)\n",
    "            self.all_kps.append(pred_kp)\n",
    "            self.all_cams.append(cams)\n",
    "            self.all_Js.append(Js)\n",
    "            # save each theta.\n",
    "            self.final_thetas.append(theta_here)\n",
    "            # Finally)update to end iteration.\n",
    "            theta_prev = theta_here\n",
    "\n",
    "\n",
    "    def prepare(self):\n",
    "        print('Restoring checkpoint %s..' % self.load_path)\n",
    "        self.saver.restore(self.sess, self.load_path)        \n",
    "        self.mean_value = self.sess.run(self.mean_var)\n",
    "            \n",
    "    def predict(self, images, get_theta=False):\n",
    "        \"\"\"\n",
    "        images: num_batch, img_size, img_size, 3\n",
    "        Preprocessed to range [-1, 1]\n",
    "        \"\"\"\n",
    "        results = self.predict_dict(images)\n",
    "        if get_theta:\n",
    "            return results['joints'], results['verts'], results['cams'], results[\n",
    "                'joints3d'], results['theta']\n",
    "        else:\n",
    "            return results['joints'], results['verts'], results['cams'], results[\n",
    "                'joints3d']\n",
    "\n",
    "    def predict_dict(self, images):\n",
    "        \"\"\"\n",
    "        images: num_batch, img_size, img_size, 3\n",
    "        Preprocessed to range [-1, 1]\n",
    "        Runs the model with images.\n",
    "        \"\"\"\n",
    "        feed_dict = {\n",
    "            self.images_pl: images,\n",
    "            # self.theta0_pl: self.mean_var,\n",
    "        }\n",
    "        fetch_dict = {\n",
    "            'joints': self.all_kps[-1],\n",
    "            'verts': self.all_verts[-1],\n",
    "            'cams': self.all_cams[-1],\n",
    "            'joints3d': self.all_Js[-1],\n",
    "            'theta': self.final_thetas[-1],\n",
    "        }\n",
    "\n",
    "        results = self.sess.run(fetch_dict, feed_dict)\n",
    "\n",
    "        # Return joints in original image space.\n",
    "        joints = results['joints']\n",
    "        results['joints'] = ((joints + 1) * 0.5) * self.img_size\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efe49d3",
   "metadata": {},
   "source": [
    "# Inference Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a929887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAMZA\\Anaconda3\\envs\\body_m\\lib\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HAMZA\\Anaconda3\\envs\\body_m\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Iteration 0\n",
      "WARNING:tensorflow:From C:\\Users\\HAMZA\\Human-Body-Measurements-using-Computer-Vision\\src\\tf_smpl\\batch_lbs.py:53: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Iteration 1\n",
      "Reuse is on!\n",
      "Iteration 2\n",
      "Reuse is on!\n",
      "Restoring checkpoint models/model.ckpt-667589..\n",
      "WARNING:tensorflow:From C:\\Users\\HAMZA\\Anaconda3\\envs\\body_m\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-667589\n",
      "$$$$$$$ (1280, 720, 3)\n",
      "height: 168.000000\n",
      "waist: 79.989816\n",
      "belly: 82.362825\n",
      "chest: 97.532834\n",
      "wrist: 17.018213\n",
      "neck: 36.913588\n",
      "arm length: 49.530642\n",
      "thigh: 50.668527\n",
      "shoulder width: 45.928664\n",
      "hips: 92.345369\n",
      "ankle: 21.173691\n",
      "Model Saved...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dir_name=input_dir;\n",
    "\n",
    "\n",
    "## setup ####################\n",
    "\n",
    "LABEL_NAMES = np.asarray([\n",
    "\t'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "\t'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "\t'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n",
    "])\n",
    "\n",
    "FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n",
    "FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)\n",
    "\n",
    "\n",
    "MODEL_NAME = 'xception_coco_voctrainval'  # @param ['mobilenetv2_coco_voctrainaug', 'mobilenetv2_coco_voctrainval', 'xception_coco_voctrainaug', 'xception_coco_voctrainval']\n",
    "\n",
    "_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
    "_MODEL_URLS = {\n",
    "\t'mobilenetv2_coco_voctrainaug':\n",
    "\t\t'deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz',\n",
    "\t'mobilenetv2_coco_voctrainval':\n",
    "\t\t'deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz',\n",
    "\t'xception_coco_voctrainaug':\n",
    "\t\t'deeplabv3_pascal_train_aug_2018_01_04.tar.gz',\n",
    "\t'xception_coco_voctrainval':\n",
    "\t\t'deeplabv3_pascal_trainval_2018_01_04.tar.gz',\n",
    "}\n",
    "_TARBALL_NAME = _MODEL_URLS[MODEL_NAME]\n",
    "\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "  tf.gfile.MakeDirs(model_dir)\n",
    "\n",
    "download_path = os.path.join(model_dir, _TARBALL_NAME)\n",
    "if not os.path.exists(download_path):\n",
    "  print('downloading model to %s, this might take a while...' % download_path)\n",
    "  urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME], \n",
    "\t\t\t     download_path)\n",
    "  print('download completed! loading DeepLab model...')\n",
    "\n",
    "MODEL = DeepLabModel(download_path)\n",
    "print('model loaded successfully!')\n",
    "\n",
    "\n",
    "image = Image.open(dir_name)\n",
    "#print(\"Image Type = \",type(image))\n",
    "back = cv2.imread('sample_data/input/background.jpeg',cv2.IMREAD_COLOR)\n",
    "\n",
    "\n",
    "res_im,seg=MODEL.run(image)\n",
    "\n",
    "seg=cv2.resize(seg.astype(np.uint8),image.size)\n",
    "mask_sel=(seg==15).astype(np.float32)\n",
    "mask = 255*mask_sel.astype(np.uint8)\n",
    "\n",
    "img = \tnp.array(image)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)   \n",
    "\n",
    "res = cv2.bitwise_and(img,img,mask = mask)\n",
    "bg_removed = res + (255 - cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)) \n",
    "main(bg_removed,height,None)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9cc6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
